# -*- coding: utf-8 -*-
"""Train_Aug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lany-9OCqk30cawWosU0vWE9RYauZcew
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

import tensorflow as tf
tf.__version__

import os
import cv2
import numpy as np

# Function to load and preprocess images
def load_images_in_batches(directory, batch_size=100):
    all_images = []
    all_labels = []

    class_names = os.listdir(directory)
    class_names.sort()  # Sort classes for consistent labeling

    for class_index, class_name in enumerate(class_names):
        class_dir = os.path.join(directory, class_name)
        if os.path.isdir(class_dir):
            image_files = os.listdir(class_dir)
            for i in range(0, len(image_files), batch_size):
                batch_files = image_files[i:i + batch_size]
                for filename in batch_files:
                    image_path = os.path.join(class_dir, filename)
                    image = cv2.imread(image_path)
                    if image is not None:
                        # Convert from BGR to Grayscale directly
                        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                        equalized_image = cv2.equalizeHist(gray_image)
                        resized_image = cv2.resize(equalized_image, (32, 32))
                        all_images.append(resized_image)
                        all_labels.append(class_index)

    return np.array(all_images), np.array(all_labels)

# Specify the correct directory path
train_directory = "/content/drive/MyDrive/TRAIN_labelledDataset"  # Replace with the correct path

# Load images from the directory
X_train, y_train = load_images_in_batches(train_directory)

# Print the shapes to confirm
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of y_train: {y_train.shape}")

unique_classes, class_counts = np.unique(y_train, return_counts=True)
print(f"Classes: {unique_classes}")
print(f"Class counts: {class_counts}")

class_names = os.listdir(train_directory)
class_names.sort()
print(f"Class names: {class_names}")

for class_name in class_names:
    class_dir = os.path.join(train_directory, class_name)
    if os.path.isdir(class_dir):
        print(f"{class_name}: {len(os.listdir(class_dir))} images")

from sklearn.model_selection import train_test_split

# First, split into training (90%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Then, further split training set into training (80%) and validation (20%)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Check the shapes of the splits
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_val: {y_val.shape}")
print(f"Shape of y_test: {y_test.shape}")

# Reshape data to add a channel dimension (LeNet-5 expects 4D input: (samples, height, width, channels))
X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)
X_val = X_val.reshape(X_val.shape[0], 32, 32, 1)
X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)

# Normalize pixel values to [0, 1]
X_train = X_train / 255.0
X_val = X_val / 255.0
X_test = X_test / 255.0

# Print new shapes to confirm
print(f"Shape of X_train after reshaping: {X_train.shape}")
print(f"Shape of X_val after reshaping: {X_val.shape}")
print(f"Shape of X_test after reshaping: {X_test.shape}")

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense

# Initialize the model
model = Sequential()

# Layer 1: Convolutional Layer
model.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(32, 32, 1)))

# Layer 2: Average Pooling Layer
model.add(AveragePooling2D(pool_size=(2, 2)))

# Layer 3: Convolutional Layer
model.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))

# Layer 4: Average Pooling Layer
model.add(AveragePooling2D(pool_size=(2, 2)))

# Layer 5: Flatten Layer
model.add(Flatten())

# Layer 6: Fully Connected Layer
model.add(Dense(120, activation='tanh'))

# Layer 7: Fully Connected Layer
model.add(Dense(84, activation='tanh'))

# Layer 8: Output Layer
model.add(Dense(12, activation='softmax'))

# Model Summary
model.summary()

from tensorflow.keras.optimizers import Adam

# Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Raw y_train shape:", y_train.shape)
print("Unique y_train values:", np.unique(y_train))

from tensorflow.keras.utils import to_categorical

# Number of classes in your dataset
num_classes = 12  # Adjust if needed

# One-hot encode the labels
y_train = to_categorical(y_train, num_classes=num_classes)
y_val = to_categorical(y_val, num_classes=num_classes)
y_test = to_categorical(y_test, num_classes=num_classes)

# Check the shape
print("y_train shape after one-hot encoding:", y_train.shape)

# Train the model
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=10,  # Adjust epochs as needed
                    batch_size=32)  # Adjust batch size if required

test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test Loss: {test_loss:.4f}")

import numpy as np
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical

# Step 1: Predict the labels for the test dataset
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Get the index of the maximum value (the predicted class)

# Step 2: If your labels are one-hot encoded, convert them back to integer labels
if y_test.shape[-1] > 1:  # If y_test is one-hot encoded
    y_true = np.argmax(y_test, axis=1)  # Convert one-hot encoded labels to integer labels
else:
    y_true = y_test  # If labels are already in integer form, use them directly

# Step 3: Generate the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_classes)

# Step 4: Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(12), yticklabels=np.arange(12))
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Save the model as augModel.h5
model.save('augModel.h5')

from google.colab import files
files.download('augModel.h5')

from google.colab import drive
drive.mount('/content/drive')

!cp augModel.h5 /content/drive/MyDrive/

import tensorflow as tf

# Load the saved Keras model
model = tf.keras.models.load_model("lenet5_augmodel.keras")

# Convert the model to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the converted model as a .tflite file
with open("lenet5_model.tflite", "wb") as f:
    f.write(tflite_model)

# Enable optimizations to quantize the model
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Convert the model with optimizations
tflite_model = converter.convert()

# Save the optimized model
with open("lenet5_model_quantized.tflite", "wb") as f:
    f.write(tflite_model)

import tensorflow as tf

# Assuming you have already converted your model into tflite_model
tflite_model = converter.convert()

# Define the file path where you want to save the model
output_path = "/content/drive/MyDrive/lenet5_augmodel.tflite"

# Save the converted model to the specified path
with open(output_path, "wb") as f:
    f.write(tflite_model)

print(f"Model saved at {output_path}")

